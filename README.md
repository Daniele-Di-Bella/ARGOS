# 🐕 ARGOS: A Retrieval-augmented GeneratiOn approach for Scientific communication

## 📚 Overview
**ARGOS** is a modular Snakemake workflow implementing a Retrieval-Augmented Generation (RAG) pipeline using documents retrieved from a local Zotero library. It automates: 
1. **Document Retrieval** from Zotero using its API.
2. **Answer Generation** and **Answer Proofreading** employing an LLM. The model now in use is OpenAI's GPT-4.1. 
3. **Evaluation** of the generated answer versus a reference. This function was added for initial tests of the app. 

## Requirements
- Python ≥ 3.8  
- Snakemake  
- Zotero with local storage and API access  
- OpenAI API key (or any other key, if the employed LLM is not an OpenAI model)

## Installation
Clone the repository and install dependencies:
```bash
git clone https://github.com/<username>/ARGOS.git
cd ARGOS
```
Create a virtual environment and install required packages: 
```bash
conda create -n ARGOS python=3.9 snakemake
conda activate ARGOS
pip install -r requirements.txt
```

## Folder structure
```bash
project_root/
│
├── Snakefile
├── config.yaml
├── .gitignore
├── README
├── LICENSE
│
├── scripts/
│   ├── API_keys.py               # Contains API credentials (Zotero key, library ID, OpenAI key)
│   ├── zotero_retriever.py       # Script to retrieve PDFs/HTMLs from Zotero
│   ├── RAG.py                    # Core Retrieval-Augmented Generation logic
│   └── evaluation.py             # Evaluation script comparing RAG output vs reference
│
├── data/
│   └── <keywords>/               # Zotero-retrieved documents (PDF/HTML) for given keyword
│
├── outputs/
│   └── <keywords>/               # Output directory for each keyword
│       ├── <question>.md            # Main RAG-generated answer
│       ├── <keywords>_RT.md         # Manually added reference answer (used in evaluation)
│       ├── <question>[Eval].md      # Markdown evaluation results
│       └── <question>[Eval].csv     # Optional CSV with evaluation metrics (if enabled)
│
└── .snakemake/                # (auto-generated by Snakemake to track workflow state)
```

## Configuration
Insert your keys in `API_keys.py` then edit the `config.yaml` file to set your parameters before running the entire pipeline. Below its an example of an extremely basic `config.yaml` file. 
```yaml
keywords: "climate change"  
language: "English"
target_audience: "Glaciologists"
question: "What are the recent trends in Arctic sea ice decline?"
model: "gpt-4.1"
vector_store_type: "faiss"
k_chunks: 50 
```
## Workflow Structure
The workflow consists of the following Snakemake rules:

1. `zotero_retrieval` -- Retrieves documents using `scripts/zotero_retriever.py` and saves files in `data/{keywords}/`.
2. `RAG` -- Embeds documents, retrieves top-k chunks, generates an answer and proofreads it. The output is saved as Markdown in `outputs/{keywords}/`
3. `evaluation` -- Compares generated output to a reference answer (`{keywords}_RT.md`). Saves results in Markdown and optionally CSV. 
4. `all` -- Ensures the complete pipeline runs end-to-end.

## Usage
Execute the pipeline with:
```bash
snakemake --cores <N> --untill <rule>
```
Replace `N` with the number of available cores and `rule` with the step of the pipeline you want to reach.

## Output Structure
Retrieved documents: `data/{keywords}/`
Generated answer: `outputs/{keywords}/{question}.md`
Evaluation report: `outputs/{keywords}/{question}[Eval].md`
Optional metrics: CSV file in the same output directory

## License & Contact
License: MIT
Author: Daniele Di Bella (daniele.dibella99@gmail.com)
